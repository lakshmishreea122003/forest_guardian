{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63e0ae74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3da884db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cad0c2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " zero_padding2d (ZeroPadding  (None, 30, 30, 1)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 28, 28, 32)        320       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 28, 28, 32)       128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " tf.nn.relu (TFOpLambda)     (None, 28, 28, 32)        0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " zero_padding2d_1 (ZeroPaddi  (None, 16, 16, 32)       0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 14, 14, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 14, 14, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " tf.nn.relu_1 (TFOpLambda)   (None, 14, 14, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 7, 7, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 7, 7, 64)          0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 5, 5, 256)         147712    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 5, 5, 256)        1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " tf.nn.relu_2 (TFOpLambda)   (None, 5, 5, 256)         0         \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 2, 2, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 2, 2, 256)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               131200    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 300,426\n",
      "Trainable params: 299,722\n",
      "Non-trainable params: 704\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data\n",
    "x_train = x_train.reshape(-1, 28, 28, 1).astype(\"float32\") / 255.0\n",
    "x_test = x_test.reshape(-1, 28, 28, 1).astype(\"float32\") / 255.0\n",
    "\n",
    "def my_model():\n",
    "    inputs = keras.layers.Input(shape=(28,28,1))\n",
    "    x = keras.layers.ZeroPadding2D(padding=(1,1),input_shape=(28,28,1))(inputs)\n",
    "    x = keras.layers.Conv2D(32,3)(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.activations.relu(x)\n",
    "    x = keras.layers.MaxPooling2D(pool_size=2,strides=2)(x)\n",
    "    x = keras.layers.ZeroPadding2D(padding=(1,1))(x)\n",
    "    x = keras.layers.Dropout(0.1)(x)\n",
    "\n",
    "    \n",
    "    x = keras.layers.Conv2D(64,3)(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.activations.relu(x)\n",
    "    x = keras.layers.MaxPooling2D(pool_size=2,strides=2)(x)\n",
    "    x = keras.layers.Dropout(0.2)(x)\n",
    "    \n",
    "\n",
    "    x = keras.layers.Conv2D(256,3)(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.activations.relu(x)\n",
    "    x = keras.layers.MaxPooling2D(pool_size=2,strides=2)(x)\n",
    "    x = keras.layers.Dropout(0.3)(x)\n",
    "    \n",
    "    \n",
    "    x = keras.layers.Flatten()(x)\n",
    "    x = keras.layers.Dense(128,activation='relu')(x)\n",
    "    outputs = keras.layers.Dense(10,activation='softmax')(x)\n",
    "    x = keras.layers.Dropout(0.4)(x)\n",
    "    \n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "model = my_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53085a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "469/469 [==============================] - 107s 224ms/step - loss: 0.1934 - accuracy: 0.9400 - val_loss: 5.1295 - val_accuracy: 0.0974\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 105s 224ms/step - loss: 0.0623 - accuracy: 0.9805 - val_loss: 15.9279 - val_accuracy: 0.0974\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 102s 218ms/step - loss: 0.0487 - accuracy: 0.9847 - val_loss: 21.9632 - val_accuracy: 0.0974\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 109s 233ms/step - loss: 0.0407 - accuracy: 0.9874 - val_loss: 21.1121 - val_accuracy: 0.1010\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 109s 232ms/step - loss: 0.0339 - accuracy: 0.9888 - val_loss: 48.7343 - val_accuracy: 0.0974\n",
      "Epoch 6/20\n",
      "219/469 [=============>................] - ETA: 1:00 - loss: 0.0318 - accuracy: 0.9895"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_32740\\2696039230.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1407\u001b[0m                 _r=1):\n\u001b[0;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1410\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2451\u001b[0m       (graph_function,\n\u001b[0;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2454\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1859\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1860\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1861\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    496\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 497\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    498\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=20, batch_size=128, verbose=1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83bdd6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1aa4bb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: digit_recognition_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: digit_recognition_model\\assets\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "model.save(r\"D:\\mlh-project\\digit_recognition_model\\digit_recognition_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1fa921cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 49ms/step\n",
      "Predicted digit: 6\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model\n",
    "# model = tf.keras.models.load_model(r\"D:\\mlh-project\\digit_recognition_model\\digit_recognition_model\")\n",
    "\n",
    "# Load and preprocess a new image\n",
    "new_image = cv2.imread(r\"C:\\Users\\Lakshmi\\Downloads\\three.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "new_image = cv2.resize(new_image, (28, 28))\n",
    "new_image = new_image.astype(\"float32\") / 255.0\n",
    "new_image = np.expand_dims(new_image, axis=0)\n",
    "new_image = np.expand_dims(new_image, axis=-1)\n",
    "\n",
    "# Perform prediction on the new image\n",
    "prediction = model.predict(new_image)\n",
    "predicted_digit = np.argmax(prediction)\n",
    "\n",
    "print(\"Predicted digit:\", predicted_digit)\n",
    "# test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "# print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b398956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9861111111111112\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['model.pkl']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "import joblib\n",
    "\n",
    "# Load the MNIST dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an SVM classifier\n",
    "clf = svm.SVC()\n",
    "\n",
    "# Train the classifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the digits for the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the classifier\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "model_path = 'model.pkl'\n",
    "joblib.dump(clf, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "520c7781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Digit: 1\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "import joblib\n",
    "\n",
    "# Load the image\n",
    "image_path = r\"C:\\Users\\Lakshmi\\Downloads\\three.jpg\"\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Preprocess the image\n",
    "image = image.convert('L')  # Convert to grayscale\n",
    "image = image.resize((8, 8))  # Resize to match MNIST image size\n",
    "image = np.array(image)  # Convert to numpy array\n",
    "image = image.flatten()  # Flatten the image\n",
    "\n",
    "# Load the trained SVM model\n",
    "model_path = r\"C:\\Users\\Lakshmi\\model.pkl\"\n",
    "clf = svm.SVC()\n",
    "clf = joblib.load(model_path)\n",
    "\n",
    "# Make a prediction\n",
    "prediction = clf.predict([image])\n",
    "\n",
    "print(\"Predicted Digit:\", prediction[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f03288f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
      "553467096/553467096 [==============================] - 335s 1us/step\n",
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 1000)              4097000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "model = VGG16(weights='imagenet')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00220e6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/images/dog.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_32740\\2743156589.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#There is an interpolation method to match the source size with the target size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#image loaded in PIL (Python Imaging Library)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolor_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'rgb'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m224\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m224\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\image_utils.py\u001b[0m in \u001b[0;36mload_img\u001b[1;34m(path, grayscale, color_mode, target_size, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[0;32m    391\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m       \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 393\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    394\u001b[0m       \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/images/dog.jpg'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input,decode_predictions\n",
    "import numpy as np\n",
    "\n",
    "img_path = '/kaggle/input/images/dog.jpg'\n",
    "#There is an interpolation method to match the source size with the target size\n",
    "#image loaded in PIL (Python Imaging Library)\n",
    "img = image.load_img(img_path,color_mode='rgb', target_size=(224, 224))\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88130bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = image.img_to_array(img)\n",
    "x.shape\n",
    "# Adding the fouth dimension, for number of images\n",
    "x = np.expand_dims(x, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea74ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean centering with respect to Image\n",
    "x = preprocess_input(x)\n",
    "features = model.predict(x)\n",
    "p = decode_predictions(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7528cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8977fd2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf297363",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85a9bdbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Plot ad hoc mnist instances\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "# load (downloaded if needed) the MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# plot 4 images as gray scale\n",
    "plt.subplot(221)\n",
    "plt.imshow(X_train[0], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(222)\n",
    "plt.imshow(X_train[1], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(223)\n",
    "plt.imshow(X_train[2], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(224)\n",
    "plt.imshow(X_train[3], cmap=plt.get_cmap('gray'))\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c8d6488c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "540f7a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb95f7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten 28*28 images to a 784 vector for each image\n",
    "num_pixels = X_train.shape[1] * X_train.shape[2]\n",
    "X_train = X_train.reshape((X_train.shape[0], num_pixels)).astype('float32')\n",
    "X_test = X_test.reshape((X_test.shape[0], num_pixels)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "131a8689",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6155f2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed41f875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_pixels, input_shape=(num_pixels,), kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ba2a7c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "300/300 - 7s - loss: 0.2790 - accuracy: 0.9204 - val_loss: 0.1324 - val_accuracy: 0.9612 - 7s/epoch - 22ms/step\n",
      "Epoch 2/10\n",
      "300/300 - 5s - loss: 0.1109 - accuracy: 0.9675 - val_loss: 0.0959 - val_accuracy: 0.9721 - 5s/epoch - 15ms/step\n",
      "Epoch 3/10\n",
      "300/300 - 4s - loss: 0.0706 - accuracy: 0.9795 - val_loss: 0.0857 - val_accuracy: 0.9742 - 4s/epoch - 14ms/step\n",
      "Epoch 4/10\n",
      "300/300 - 4s - loss: 0.0500 - accuracy: 0.9856 - val_loss: 0.0667 - val_accuracy: 0.9790 - 4s/epoch - 15ms/step\n",
      "Epoch 5/10\n",
      "300/300 - 6s - loss: 0.0365 - accuracy: 0.9894 - val_loss: 0.0650 - val_accuracy: 0.9793 - 6s/epoch - 18ms/step\n",
      "Epoch 6/10\n",
      "300/300 - 5s - loss: 0.0260 - accuracy: 0.9933 - val_loss: 0.0654 - val_accuracy: 0.9786 - 5s/epoch - 18ms/step\n",
      "Epoch 7/10\n",
      "300/300 - 5s - loss: 0.0187 - accuracy: 0.9957 - val_loss: 0.0581 - val_accuracy: 0.9815 - 5s/epoch - 16ms/step\n",
      "Epoch 8/10\n",
      "300/300 - 6s - loss: 0.0143 - accuracy: 0.9967 - val_loss: 0.0690 - val_accuracy: 0.9786 - 6s/epoch - 19ms/step\n",
      "Epoch 9/10\n",
      "300/300 - 5s - loss: 0.0114 - accuracy: 0.9976 - val_loss: 0.0603 - val_accuracy: 0.9820 - 5s/epoch - 16ms/step\n",
      "Epoch 10/10\n",
      "300/300 - 4s - loss: 0.0074 - accuracy: 0.9988 - val_loss: 0.0599 - val_accuracy: 0.9808 - 4s/epoch - 15ms/step\n",
      "Baseline Error: 1.92%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# build the model\n",
    "model = baseline_model()\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aad6b5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Lakshmi\\mlh-models\\assets\n",
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# model.save( r\"C:\\Users\\Lakshmi\\mlh-models\")\n",
    "# print(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bc8cb5f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Nothing to load. No dependencies have been added to <keras.engine.sequential.Sequential object at 0x0000027CD7A3ABE0> yet.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_32740\\1094759206.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mloaded_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mloaded_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"C:\\Users\\Lakshmi\\mlh-models\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Model loaded successfully.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\util.py\u001b[0m in \u001b[0;36massert_nontrivial_match\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    872\u001b[0m             f\"checkpointed value: {list(unused_python_objects)}\")\n\u001b[0;32m    873\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 874\u001b[1;33m         raise AssertionError(\n\u001b[0m\u001b[0;32m    875\u001b[0m             \u001b[1;34m\"Nothing to load. No dependencies have been added to \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m             f\"{self._object_graph_view.root} yet.\")\n",
      "\u001b[1;31mAssertionError\u001b[0m: Nothing to load. No dependencies have been added to <keras.engine.sequential.Sequential object at 0x0000027CD7A3ABE0> yet."
     ]
    }
   ],
   "source": [
    "# loaded_model = Sequential()\n",
    "# loaded_model = loaded_model.load_weights(r\"C:\\Users\\Lakshmi\\mlh-models\")\n",
    "# print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9872136f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 66ms/step\n",
      "Predicted Digit: 5\n"
     ]
    }
   ],
   "source": [
    "digit_image = Image.open(r\"C:\\Users\\Lakshmi\\Downloads\\two.jpg\").convert(\"L\")  # Convert to grayscale\n",
    "digit_image = digit_image.resize((28, 28))  # Resize to 28x28\n",
    "digit_array = np.array(digit_image)\n",
    "digit_array = digit_array.reshape(1, num_pixels).astype('float32')\n",
    "digit_array = digit_array / 255\n",
    "\n",
    "# Use the loaded model to predict the digit\n",
    "prediction = model.predict(digit_array)\n",
    "predicted_digit = np.argmax(prediction)\n",
    "\n",
    "print(\"Predicted Digit:\", predicted_digit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "171a97ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 62ms/step\n",
      "Predicted Digit: 5\n"
     ]
    }
   ],
   "source": [
    "digit_image = Image.open(r\"C:\\Users\\Lakshmi\\Downloads\\two.jpg\").convert(\"L\")  # Convert to grayscale\n",
    "digit_image = digit_image.resize((28, 28))  # Resize to 28x28\n",
    "digit_array = np.array(digit_image)\n",
    "digit_array = digit_array.reshape(1, num_pixels).astype('float32')\n",
    "digit_array = digit_array / 255\n",
    "\n",
    "# Use the loaded model to predict the digit\n",
    "prediction = model.predict(digit_array)\n",
    "predicted_digit = np.argmax(prediction)\n",
    "\n",
    "print(\"Predicted Digit:\", predicted_digit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea247d66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190e98b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ce0e884f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "300/300 - 3s - loss: 0.2801 - accuracy: 0.9195 - val_loss: 0.1400 - val_accuracy: 0.9605 - 3s/epoch - 10ms/step\n",
      "Epoch 2/10\n",
      "300/300 - 2s - loss: 0.1112 - accuracy: 0.9681 - val_loss: 0.0998 - val_accuracy: 0.9696 - 2s/epoch - 7ms/step\n",
      "Epoch 3/10\n",
      "300/300 - 2s - loss: 0.0723 - accuracy: 0.9791 - val_loss: 0.0758 - val_accuracy: 0.9764 - 2s/epoch - 6ms/step\n",
      "Epoch 4/10\n",
      "300/300 - 2s - loss: 0.0506 - accuracy: 0.9848 - val_loss: 0.0686 - val_accuracy: 0.9782 - 2s/epoch - 6ms/step\n",
      "Epoch 5/10\n",
      "300/300 - 2s - loss: 0.0369 - accuracy: 0.9894 - val_loss: 0.0626 - val_accuracy: 0.9805 - 2s/epoch - 6ms/step\n",
      "Epoch 6/10\n",
      "300/300 - 2s - loss: 0.0273 - accuracy: 0.9925 - val_loss: 0.0637 - val_accuracy: 0.9799 - 2s/epoch - 6ms/step\n",
      "Epoch 7/10\n",
      "300/300 - 2s - loss: 0.0196 - accuracy: 0.9949 - val_loss: 0.0603 - val_accuracy: 0.9804 - 2s/epoch - 7ms/step\n",
      "Epoch 8/10\n",
      "300/300 - 2s - loss: 0.0141 - accuracy: 0.9969 - val_loss: 0.0612 - val_accuracy: 0.9811 - 2s/epoch - 6ms/step\n",
      "Epoch 9/10\n",
      "300/300 - 2s - loss: 0.0108 - accuracy: 0.9979 - val_loss: 0.0564 - val_accuracy: 0.9816 - 2s/epoch - 6ms/step\n",
      "Epoch 10/10\n",
      "300/300 - 2s - loss: 0.0080 - accuracy: 0.9984 - val_loss: 0.0661 - val_accuracy: 0.9806 - 2s/epoch - 6ms/step\n",
      "Baseline Error: 1.94%\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# flatten 28*28 images to a 784 vector for each image\n",
    "num_pixels = X_train.shape[1] * X_train.shape[2]\n",
    "X_train = X_train.reshape((X_train.shape[0], num_pixels)).astype('float32')\n",
    "X_test = X_test.reshape((X_test.shape[0], num_pixels)).astype('float32')\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "# one hot encode outputs\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]\n",
    "# define baseline model\n",
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_pixels, input_shape=(num_pixels,), kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "# build the model\n",
    "model = baseline_model()\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d253803",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60536f72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb416f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48484941",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120375fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207f8c5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7262b033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30fc223e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cdb5b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to C:\\Users\\Lakshmi/.cache\\torch\\hub\\v0.10.0.zip\n",
      "C:\\Users\\Lakshmi\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lakshmi\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to C:\\Users\\Lakshmi/.cache\\torch\\hub\\checkpoints\\vgg19-dcbb9e9d.pth\n",
      "100%|| 548M/548M [04:20<00:00, 2.21MB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg19', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59b72305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): ReLU(inplace=True)\n",
       "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (33): ReLU(inplace=True)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): ReLU(inplace=True)\n",
       "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79074838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9912422/9912422 [00:03<00:00, 2591581.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 28881/28881 [00:00<00:00, 12889518.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1648877/1648877 [00:00<00:00, 3371944.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 4542/4542 [00:00<00:00, 4564094.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MNIST(root=\"./data\", train=True, transform=ToTensor(), download=True)\n",
    "test_dataset = MNIST(root=\"./data\", train=False, transform=ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "109ad2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model = VGG19().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "105cdf6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 3, 3, 3], expected input[64, 1, 28, 28] to have 3 channels, but got 1 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3616\\3701367350.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\models\\vgg.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 463\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 459\u001b[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 3, 3, 3], expected input[64, 1, 28, 28] to have 3 channels, but got 1 channels instead"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d72a06f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cb181f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the final model to file\n",
    "# from tensorflow.keras.datasets import mnist\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Conv2D\n",
    "# from tensorflow.keras.layers import MaxPooling2D\n",
    "# from tensorflow.keras.layers import Dense\n",
    "# from tensorflow.keras.layers import Flatten\n",
    "# from tensorflow.keras.optimizers import SGD\n",
    " \n",
    "# # load train and test dataset\n",
    "# def load_dataset():\n",
    "#     (trainX, trainY), (testX, testY) = mnist.load_data()\n",
    "#  # reshape dataset to have a single channel\n",
    "#     trainX = trainX.reshape((trainX.shape[0], 28, 28, 1))\n",
    "#     testX = testX.reshape((testX.shape[0], 28, 28, 1))\n",
    "#  # one hot encode target values\n",
    "#     trainY = to_categorical(trainY)\n",
    "#     testY = to_categorical(testY)\n",
    "#     return trainX, trainY, testX, testY\n",
    " \n",
    "# # scale pixels\n",
    "# def prep_pixels(train, test):\n",
    "#     train_norm = train.astype('float32')\n",
    "#     test_norm = test.astype('float32')\n",
    "#  # normalize to range 0-1\n",
    "#     train_norm = train_norm / 255.0\n",
    "#     test_norm = test_norm / 255.0\n",
    "#  # return normalized images\n",
    "#     return train_norm, test_norm\n",
    " \n",
    "# # define cnn model\n",
    "# def define_model():\n",
    "#     model = Sequential()\n",
    "#     model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\n",
    "#     model.add(MaxPooling2D((2, 2)))\n",
    "#     model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "#     model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "#     model.add(MaxPooling2D((2, 2)))\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n",
    "#     model.add(Dense(10, activation='softmax'))\n",
    "#  # compile model\n",
    "#     opt = SGD(learning_rate=0.01, momentum=0.9)\n",
    "#     model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#     return model\n",
    " \n",
    "# # run the test harness for evaluating a model\n",
    "# def run_test_harness():\n",
    "#     trainX, trainY, testX, testY = load_dataset()\n",
    "#     trainX, testX = prep_pixels(trainX, testX)\n",
    "#     model = define_model()\n",
    "#     model.fit(trainX, trainY, epochs=10, batch_size=32, verbose=0)\n",
    "#     model.save('final_model.h5')\n",
    " \n",
    "\n",
    "# run_test_harness()\n",
    "\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "from tensorflow.keras.optimizers import SGD\n",
    " \n",
    "# load train and test dataset\n",
    "def load_dataset():\n",
    "    (trainX, trainY), (testX, testY) = mnist.load_data()\n",
    "    # reshape dataset to have a single channel\n",
    "    trainX = trainX.reshape((trainX.shape[0], 28, 28, 1))\n",
    "    testX = testX.reshape((testX.shape[0], 28, 28, 1))\n",
    "    # one hot encode target values\n",
    "    trainY = to_categorical(trainY)\n",
    "    testY = to_categorical(testY)\n",
    "    return trainX, trainY, testX, testY\n",
    " \n",
    "# scale pixels\n",
    "def prep_pixels(train, test):\n",
    "    train_norm = train.astype('float32')\n",
    "    test_norm = test.astype('float32')\n",
    "    # normalize to range 0-1\n",
    "    train_norm = train_norm / 255.0\n",
    "    test_norm = test_norm / 255.0\n",
    "    # return normalized images\n",
    "    return train_norm, test_norm\n",
    " \n",
    "# define cnn model\n",
    "def define_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    # compile model\n",
    "    opt = SGD(learning_rate=0.01, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    " \n",
    "# run the test harness for evaluating a model\n",
    "def run_test_harness():\n",
    "    trainX, trainY, testX, testY = load_dataset()\n",
    "    print(\"load completed\")\n",
    "    trainX, testX = prep_pixels(trainX, testX)\n",
    "    print(\"prepare pixels completed\")\n",
    "    model = define_model()\n",
    "    print(\"model defined\")\n",
    "    model.fit(trainX, trainY, epochs=10, batch_size=32, verbose=0)\n",
    "    print(\"model fit\")\n",
    "    model.save('final_model.h5')\n",
    "    print(\"model saved\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68c21738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load completed\n",
      "prepare pixels completed\n",
      "model defined\n",
      "model fit\n",
      "model saved\n"
     ]
    }
   ],
   "source": [
    "run_test_harness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fcf8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"C:\\Users\\Lakshmi\\Downloads\\seven.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72877b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction for a new image.\n",
    "from numpy import argmax\n",
    "from keras.utils import load_img\n",
    "from keras.utils import img_to_array\n",
    "from keras.models import load_model\n",
    " \n",
    "# load and prepare the image\n",
    "def load_image(filename):\n",
    "    img = load_img(filename, grayscale=True, target_size=(28, 28))\n",
    "    img = img_to_array(img)\n",
    " # reshape into a single sample with 1 channel\n",
    "    img = img.reshape(1, 28, 28, 1)\n",
    " # prepare pixel data\n",
    "    img = img.astype('float32')\n",
    "    img = img / 255.0\n",
    "    return img\n",
    " \n",
    "# load an image and predict the class\n",
    "def run_example():\n",
    "    img = load_image(r\"C:\\Users\\Lakshmi\\Downloads\\nine-draw.jpg\")\n",
    " # load model\n",
    "    model = load_model('final_model.h5')\n",
    "    predict_value = model.predict(img)\n",
    "    digit = argmax(predict_value)\n",
    "    print(digit)\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac25aa66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 253ms/step\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "run_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be11c76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea12b283",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "from keras.utils import load_img\n",
    "from keras.utils import img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75655f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a86d6c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    " model = load_model('final_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "475d132f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3296\\931681315.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"C:\\Users\\Lakshmi\\Downloads\\seven-draw.jpg\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mpredict_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdigit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdigit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_image' is not defined"
     ]
    }
   ],
   "source": [
    "img = load_image(r\"C:\\Users\\Lakshmi\\Downloads\\seven-draw.jpg\")\n",
    "predict_value = model.predict(img)\n",
    "digit = argmax(predict_value)\n",
    "print(digit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9e4ec0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
